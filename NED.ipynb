{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6062f3-4454-4368-87a0-c2b2c28b1460",
   "metadata": {},
   "source": [
    "# NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "356b1314-9837-419f-a266-17ac953048fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astroquery.ipac.ned import Ned\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_ned(\n",
    "    dataframe,\n",
    "    limit_distance_mpc=50,\n",
    "    output_prefix=None,\n",
    "    batch_size=10,\n",
    "    max_workers=5,\n",
    "    sleep_between_batches=5,\n",
    "    H0=70.0,\n",
    "    ri_method='median',\n",
    "    csv_path_for_ri=None,\n",
    "    split_by_band=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallelized, batch-based NED extragalactic search and constraint pipeline for field-based SETI analysis.\n",
    "    Designed to follow galaxy-targeted methods in Garrett & Siemion (2023) and Enriquez et al. (2018).\n",
    "    \n",
    "    Each field corresponds to a cone search around (RA, Dec) with given beam size.\n",
    "    Detections from the NASA/IPAC Extragalactic Database (NED) are queried per beam.\n",
    "    For each object, radiative constraints (EIRPmin), population estimates (N_stars), and Galaxy-type-dependent completeness corrections are computed.\n",
    "    \n",
    "    This function supports per-band and per-field batch analysis as in G&S 2023, with optional redshift-independent distance substitution (NED-D).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        Must contain the following columns:\n",
    "        - 'ra' [deg]: Right Ascension of beam center (ICRS, degrees)\n",
    "        - 'dec' [deg]: Declination of beam center (ICRS, degrees)\n",
    "        - 'fwhm_arcmin' [arcmin]: Full Width at Half Maximum of beam (Gaussians assumed)\n",
    "        - 'fmin' [W * m^−2]: Band-dependent minimum detectable flux density \n",
    "        - 'nu_rel' [unitless]: total bandwidth of the receiver normalized by the central \n",
    "            observing frequency (used in Transmitter Rate and CWTFM scaling)\n",
    "        - 'field name': Unique field/mosaic name (converted to string internally)\n",
    "        - 'receiving_band': Observation band (e.g., 'L', 'S') used in multi-band analysis\n",
    "    \n",
    "    limit_distance_mpc : float\n",
    "        Maximum comoving distance (in Mpc) for catalog inclusion, e.g., 50 Mpc by default\n",
    "    \n",
    "    output_prefix : str or None, optional\n",
    "        If provided, batch-level results and summary files are saved as CSV using this prefix\n",
    "    \n",
    "    batch_size : int, optional\n",
    "        Number of fields grouped per processing batch (default: 10). Each batch is processed independently in parallel.\n",
    "    \n",
    "    max_workers : int, optional\n",
    "        Number of parallel worker threads used for concurrent NED queries (default: 5).\n",
    "    \n",
    "    sleep_between_batches : float, optional\n",
    "        Seconds to pause between launching new batches (avoids API throttling by NED)\n",
    "    \n",
    "    H0 : float, optional\n",
    "        Hubble constant [km/s/Mpc] for converting redshift to distance if redshift-independent estimates unavailable (default: 70)\n",
    "    \n",
    "    ri_method : {'median', 'mean', 'all'}, optional\n",
    "        Aggregation mode for NED-D redshift-independent distance estimates:\n",
    "        - 'median': Use median of all NED-D distances per object\n",
    "        - 'mean': Use arithmetic mean\n",
    "        - 'all': Expand object into multiple entries, one per available distance\n",
    "    \n",
    "    csv_path_for_ri : str or None, optional\n",
    "        If provided, path to a local CSV of the NED-D dataset is used instead of querying GitHub-hosted defaults.\n",
    "    \n",
    "    split_by_band : bool, optional (default: False)\n",
    "        If True, results are returned as per-receiving band dictionaries.\n",
    "        If False, all fields are processed in one combined set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    If split_by_band == False:\n",
    "        ned_df : pd.DataFrame\n",
    "            Concatenated table of all extragalactic objects queried across all fields, with beam scaling, frequency band, and per-object metadata.\n",
    "        type_counts_df : pd.DataFrame\n",
    "            A table of galaxy/object Type frequencies, useful for overview and diagnostics.\n",
    "        summary_df : pd.DataFrame\n",
    "            Table of unique extragalactic objects with:\n",
    "            - EIRPmin limits [W]\n",
    "            - Transmitter Rate log(TR) constraints\n",
    "            - CWTFM values\n",
    "    \n",
    "    If split_by_band == True:\n",
    "        Returns dictionaries:\n",
    "            ned_df_dict : {band_name: pd.DataFrame}\n",
    "            type_counts_df_dict : {band_name: pd.DataFrame}\n",
    "            summary_df_dict : {band_name: pd.DataFrame}\n",
    "        Access each DataFrame by ned_df_dict['band 1']...,\n",
    "        type_counts_df_dict['band 1']...\n",
    "        summary_df_dict['band 1']\n",
    "    \n",
    "    Units\n",
    "    -----\n",
    "    - RA, Dec: degrees\n",
    "    - fwhm_arcmin: arcminutes\n",
    "    - fmin: Watts per meter squared [W * m^−2]\n",
    "    - nu_rel(BW/f): unitless, where f represents the observed frequency, \n",
    "        and BW denotes the total bandwidth in GHz\n",
    "    - distance_mpc: Mpc\n",
    "    - scaling_factor: beam response [%] using Gaussian form: exp(−4ln(2) (θ / FWHM)^2)\n",
    "    - EIRPmin: Watts [W]\n",
    "    - log_EIRPmin: log10(W)\n",
    "    - log_TR: log10(Transmitter Rate)\n",
    "    - CWTFM: Continuous Waveform Transmitter Figure-of-Merit [unitless]\n",
    "    \n",
    "    Key Calculations (from G&S 2023) \n",
    "    -------------------------------\n",
    "    - **Beam response**: Gaussian beam attenuation based on offset θ from center to object\n",
    "    - **EIRPmin**:\n",
    "        log₁₀(EIRPmin) = log₁₀(4π) + 2 log₁₀(d) + log₁₀(fmin) − log₁₀(response_frac)\n",
    "        where d in meters, fmin in W * m^−2, response_frac is beam scaling factor (normalized to 1)\n",
    "    - **Transmitter Rate (TR)**:\n",
    "        log_TR = − log₁₀(N_stars) − log₁₀(ν_rel)\n",
    "    - **N_stars**:\n",
    "        - Galaxy: 1e11 assumed\n",
    "        - Group/Cluster: Estimated from velocity dispersion, and if redshift NaN in NED-D and NED, then estimated from\n",
    "            catalog designation, e.g., Clusters assigned 10 galaxies\n",
    "        - Uncertainties propagated based on galaxy counts\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Results follow methodology introduced in Garrett & Siemion (2023)\n",
    "    - Objects may appear multiple times across fields but are uniquely grouped for summary constraints.\n",
    "    - Uses both redshift-independent distances (NED-D) and cosmological distances (cz / H0) where needed\n",
    "    - Redshift limits applied: z ∈ [0.01, 0.1] for cz/H0 approximation to remain valid\n",
    "    - Beam falloff is computed using the angular offset and input FWHM in arcminutes\n",
    "    - Each receiving band is expected to have uniform ν_rel and fmin values across fields (used for CWTFM)\n",
    "    - Summary table removes duplicates and produces one constraint per extragalactic object\n",
    "    \n",
    "    Citations\n",
    "    ---------\n",
    "    - M A Garrett, A P V Siemion, Constraints on extragalactic transmitters via Breakthrough Listen observations \n",
    "        of background sources, Monthly Notices of the Royal Astronomical Society, Volume 519, Issue 3, March 2023, \n",
    "        Pages 4581–4588, https://doi.org/10.1093/mnras/stac2607\n",
    "    - B S Wlodarczyk-Sroka, M A Garrett, A P V Siemion, Extending the Breakthrough Listen nearby star survey to \n",
    "        other stellar objects in the field, Monthly Notices of the Royal Astronomical Society, Volume 498, Issue 4, \n",
    "        November 2020, Pages 5720–5729, https://doi.org/10.1093/mnras/staa2672\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe.loc[:, 'field name'] = dataframe.loc[:, 'field name'].astype(str)\n",
    "    dataframe.loc[:, 'receiving_band'] = dataframe.loc[:, 'receiving_band'].astype(str)\n",
    "    dataframe.loc[:, 'ra'] = dataframe.loc[:, 'ra'].astype(float)\n",
    "    dataframe.loc[:, 'dec'] = dataframe.loc[:, 'dec'].astype(float)\n",
    "    dataframe.loc[:, 'fwhm_arcmin'] = dataframe.loc[:, 'fwhm_arcmin'].astype(float)\n",
    "    dataframe.loc[:, 'fmin'] = dataframe.loc[:, 'fmin'].astype(float)\n",
    "    dataframe.loc[:, 'nu_rel'] = dataframe.loc[:, 'nu_rel'].astype(float)\n",
    "\n",
    "    # Batch-cone-search and combine results for a given fields subset\n",
    "    def ned_batch_query(fields_df, band_name, output_prefix=None):\n",
    "        n_total = len(fields_df)\n",
    "        batch_files = []\n",
    "\n",
    "        def angular_separation_arcmin(beam_ra, beam_dec, star_ra, star_dec):\n",
    "            c1 = SkyCoord(beam_ra, beam_dec, unit='deg')\n",
    "            c2 = SkyCoord(star_ra, star_dec, unit='deg')\n",
    "            return c1.separation(c2).arcminute\n",
    "\n",
    "        def beam_scaling_percent(beam_ra, beam_dec, star_ra, star_dec, fwhm_arcmin):\n",
    "            theta_arcmin = angular_separation_arcmin(beam_ra, beam_dec, star_ra, star_dec)\n",
    "            exponent = -4 * np.log(2) * (theta_arcmin / fwhm_arcmin)**2\n",
    "            scaling_factor = np.exp(exponent) * 100  # percent\n",
    "            return theta_arcmin, scaling_factor\n",
    "\n",
    "        def process_beam_vectorized(beam_ra, beam_dec, fwhm_arcmin, catalog_df):\n",
    "            star_ras = catalog_df['ra'].to_numpy()\n",
    "            star_decs = catalog_df['dec'].to_numpy()\n",
    "            theta, scaling = beam_scaling_percent(beam_ra, beam_dec, star_ras, star_decs, fwhm_arcmin)\n",
    "            catalog_df = catalog_df.copy()\n",
    "            catalog_df['theta_arcmin'] = theta\n",
    "            catalog_df['scaling_factor'] = scaling\n",
    "            return catalog_df\n",
    "\n",
    "        def fetch_ned_batch(batch_indices, batch_idx):\n",
    "            batch_results = []\n",
    "            for i in batch_indices:\n",
    "                row = fields_df.iloc[i]\n",
    "                ra = float(row['ra'])\n",
    "                dec = float(row['dec'])\n",
    "                fwhm = float(row['fwhm_arcmin'])\n",
    "                field_name = str(row['field name'])\n",
    "                fmin = float(row['fmin'])\n",
    "                nu_rel = float(row['nu_rel'])\n",
    "                try:\n",
    "                    result_table = Ned.query_region(\n",
    "                        SkyCoord(ra=ra * u.deg, dec=dec * u.deg),\n",
    "                        radius=(fwhm / 2.0) * u.arcmin)\n",
    "                    df = result_table.to_pandas()\n",
    "                    df = df.rename(columns={\"RA\": \"ra\", \"DEC\": \"dec\"})\n",
    "                    df['field_name'] = field_name\n",
    "                    df['fmin'] = fmin\n",
    "                    df['nu_rel'] = nu_rel\n",
    "                    df['fwhm_arcmin'] = fwhm\n",
    "                    df = process_beam_vectorized(ra, dec, fwhm, df)\n",
    "                    batch_results.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error querying NED for RA={ra}, Dec={dec}, FWHM={fwhm}: {e}\")\n",
    "            if batch_results:\n",
    "                batch_df = pd.concat(batch_results, ignore_index=True)\n",
    "                if output_prefix:\n",
    "                    filename = f\"{output_prefix}_NED_{band_name}_batch{batch_idx}.csv\"\n",
    "                    batch_df.to_csv(filename, index=False)\n",
    "                    return filename\n",
    "                else:\n",
    "                    return batch_df\n",
    "            return None\n",
    "\n",
    "        batch_results = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for batch_start in range(0, n_total, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, n_total)\n",
    "                batch_indices = list(range(batch_start, batch_end))\n",
    "                futures.append(executor.submit(fetch_ned_batch, batch_indices, batch_start // batch_size))\n",
    "                time.sleep(sleep_between_batches)\n",
    "            for future in as_completed(futures):\n",
    "                res = future.result()\n",
    "                if res is not None:\n",
    "                    batch_results.append(res)\n",
    "\n",
    "        if not batch_results:\n",
    "            print(\"No NED batch results fetched.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if output_prefix:\n",
    "            dfs = [pd.read_csv(f) for f in batch_results]\n",
    "        else:\n",
    "            dfs = batch_results\n",
    "        ned_df = pd.concat(dfs, ignore_index=True)\n",
    "        # Clean up columns\n",
    "        if 'eirp_boost' in ned_df.columns:\n",
    "            ned_df = ned_df.drop(columns=['eirp_boost'])\n",
    "        for c in ['input_ra', 'input_dec', 'input_fwhm_arcmin', 'd_max_mpc']:\n",
    "            if c in ned_df.columns:\n",
    "                ned_df = ned_df.drop(columns=c)\n",
    "        ned_df = ned_df.drop_duplicates(subset=['Object Name'])\n",
    "        col_to_move = ned_df.pop('field_name')\n",
    "        ned_df.insert(0, 'field_name', col_to_move)\n",
    "        type_col = 'Type' if 'Type' in ned_df.columns else 'type'\n",
    "        unique_types, type_counts = np.unique(ned_df[type_col].fillna('Unknown'), return_counts=True)\n",
    "        type_counts_df = pd.DataFrame({'Type': unique_types, 'Count': type_counts})\n",
    "        return ned_df, type_counts_df\n",
    "\n",
    "    # Catalog summary (deduplication, grouping, EIRPmin/constraints)\n",
    "    def ned_catalog_summary(\n",
    "        ned_df,\n",
    "        limit_distance_mpc,\n",
    "        output_prefix=None,\n",
    "        H0=70.0,\n",
    "        ri_method='median',\n",
    "        csv_path_for_ri=None\n",
    "    ):\n",
    "        def get_ned_redshift_independent_distances(csv_path=None):\n",
    "            url = \"https://ned.ipac.caltech.edu/Archive/Distances/NED30.5.1-D-17.1.2-20200415.csv\"\n",
    "            column_names = [\n",
    "                \"Exclusion Code\", \"Record index\", \"Object index\", \"Galaxy ID\",\n",
    "                \"m-M\", \"err\", \"D (Mpc)\", \"Method\", \"REFCODE\", \"SN ID\",\n",
    "                \"redshift (z)\", \"Hubble const.\", \"Adopted LMC modulus\",\n",
    "                \"Date (Yr. - 1980)\", \"Notes\"\n",
    "            ]\n",
    "            if csv_path is None:\n",
    "                ned_data = pd.read_csv(url, names=column_names, header=None, low_memory=False)\n",
    "            else:\n",
    "                ned_data = pd.read_csv(csv_path, names=column_names, header=None, low_memory=False)\n",
    "            return ned_data\n",
    "\n",
    "        ned_df = ned_df.copy()\n",
    "        ned_df['field name'] = ned_df['field_name'].astype(str)\n",
    "        ned_data = get_ned_redshift_independent_distances(csv_path=csv_path_for_ri)\n",
    "\n",
    "        ri_dist_dict = defaultdict(list)\n",
    "        ri_z_dict = defaultdict(list)\n",
    "        for _, row in ned_data.iterrows():\n",
    "            galid = str(row['Galaxy ID']).strip()\n",
    "            dmpc = pd.to_numeric(row['D (Mpc)'], errors='coerce')\n",
    "            zri = pd.to_numeric(row['redshift (z)'], errors='coerce')\n",
    "            if not pd.isnull(dmpc):\n",
    "                ri_dist_dict[galid].append(dmpc)\n",
    "            if not pd.isnull(zri):\n",
    "                ri_z_dict[galid].append(zri)\n",
    "        # Add/combine NED-D for all objects\n",
    "        expanded_rows = []\n",
    "        for _, row in ned_df.iterrows():\n",
    "            obj = row['Object Name']\n",
    "            has_ri_dist = obj in ri_dist_dict and len(ri_dist_dict[obj]) > 0\n",
    "            has_ri_z = obj in ri_z_dict and len(ri_z_dict[obj]) > 0\n",
    "            if has_ri_dist or has_ri_z:\n",
    "                dvals = ri_dist_dict[obj] if has_ri_dist else []\n",
    "                zvals = ri_z_dict[obj] if has_ri_z else []\n",
    "                if ri_method == \"median\":\n",
    "                    dval = np.median(dvals) if dvals else np.nan\n",
    "                    derr = 0.741 * (np.percentile(dvals, 75)-np.percentile(dvals, 25)) if len(dvals)>1 else np.nan\n",
    "                    zval = np.median(zvals) if zvals else np.nan\n",
    "                    new_row = row.copy()\n",
    "                    new_row['distance_mpc'], new_row['distance_mpc_err'] = dval, derr\n",
    "                    new_row['Redshift'] = zval if not pd.isnull(zval) else row['Redshift']\n",
    "                    new_row['distance_method'] = 'NED-RI Median'\n",
    "                    expanded_rows.append(new_row)\n",
    "                elif ri_method == \"mean\":\n",
    "                    dval = np.mean(dvals) if dvals else np.nan\n",
    "                    derr = np.std(dvals) if len(dvals)>1 else np.nan\n",
    "                    zval = np.mean(zvals) if zvals else np.nan\n",
    "                    new_row = row.copy()\n",
    "                    new_row['distance_mpc'], new_row['distance_mpc_err'] = dval, derr\n",
    "                    new_row['Redshift'] = zval if not pd.isnull(zval) else row['Redshift']\n",
    "                    new_row['distance_method'] = 'NED-RI Mean'\n",
    "                    expanded_rows.append(new_row)\n",
    "                elif ri_method == \"all\":\n",
    "                    L = max(len(dvals), len(zvals), 1)\n",
    "                    for k in range(L):\n",
    "                        new_row = row.copy()\n",
    "                        new_row['distance_mpc'] = dvals[k] if k < len(dvals) else np.nan\n",
    "                        new_row['distance_mpc_err'] = np.nan\n",
    "                        new_row['Redshift'] = zvals[k] if k < len(zvals) and not pd.isnull(zvals[k]) else row['Redshift']\n",
    "                        new_row['distance_method'] = 'NED-RI Single'\n",
    "                        expanded_rows.append(new_row)\n",
    "                else:\n",
    "                    expanded_rows.append(row)\n",
    "            else:\n",
    "                expanded_rows.append(row)\n",
    "        ned_df = pd.DataFrame(expanded_rows)\n",
    "        # Assign cz/H0 distance, if NED-D absent\n",
    "        c_kms = 299792.458\n",
    "        def assign_distance(row):\n",
    "            if 'distance_mpc' in row and not pd.isnull(row['distance_mpc']):\n",
    "                return row\n",
    "            z = row.get('Redshift', np.nan)\n",
    "            if pd.isnull(z) or z < 0 or z < 0.01 or z >= 0.1:\n",
    "                row['distance_mpc'] = np.nan\n",
    "                return row\n",
    "            row['distance_mpc'] = (z * c_kms) / H0\n",
    "            return row\n",
    "        ned_df = ned_df.apply(assign_distance, axis=1)\n",
    "        # Membership, deduplication, grouping\n",
    "        redshift_window_params = dict(pair=500, triple=600, group=1000, cluster=1500)\n",
    "        group_names = {'pair': 'GPair', 'triple': 'GTrpl', 'group': 'GGroup', 'cluster': 'GClstr'}\n",
    "        group_membership = defaultdict(set)\n",
    "        for kind, tag in group_names.items():\n",
    "            disp = redshift_window_params[kind]\n",
    "            groups = ned_df[(ned_df['Type'] == tag) & (ned_df['Redshift'].notnull())]\n",
    "            for _, group in groups.iterrows():\n",
    "                z0, ra0, dec0 = group['Redshift'], group['ra'], group['dec']\n",
    "                dz = disp / c_kms\n",
    "                zmin, zmax = z0 - dz, z0 + dz\n",
    "                radius_deg = group.get('fwhm_arcmin', 3.0) / 2.0 / 60.0\n",
    "                gcenter = SkyCoord(ra0 * u.deg, dec0 * u.deg)\n",
    "                valid = (\n",
    "                    ned_df['ra'].notnull() & ned_df['dec'].notnull() &\n",
    "                    np.isfinite(ned_df['ra']) & np.isfinite(ned_df['dec'])\n",
    "                )\n",
    "                if not valid.all():\n",
    "                    n_bad = (~valid).sum()\n",
    "                    print(f\"Warning: Removing {n_bad} objects with invalid RA/DEC for SkyCoord.\")\n",
    "                # If you only want to filter for the SkyCoord step:\n",
    "                ra_arr = ned_df.loc[valid, 'ra'].astype(float).values\n",
    "                dec_arr = ned_df.loc[valid, 'dec'].astype(float).values\n",
    "                all_coords = SkyCoord(ra_arr * u.deg, dec_arr * u.deg)\n",
    "                separation = gcenter.separation(all_coords).deg\n",
    "                members = ned_df[\n",
    "                    (separation <= radius_deg) &\n",
    "                    (ned_df['Redshift'] >= zmin) & (ned_df['Redshift'] <= zmax) &\n",
    "                    (ned_df['Type'] == 'G')\n",
    "                ]\n",
    "                member_names = set(members['Object Name'])\n",
    "                group_membership[group['Object Name']] |= member_names\n",
    "\n",
    "        def cluster_nstar_error(n_gal):\n",
    "            if n_gal < 10:\n",
    "                return 0.3\n",
    "            return 0.5 + 0.2 * max(0, np.floor(np.log10(n_gal)) - 1)\n",
    "\n",
    "        def get_nstars(row):\n",
    "            obj = row.get('Object Name')\n",
    "            t = row.get('Type', '')\n",
    "            if obj in group_membership and len(group_membership[obj]) > 0:\n",
    "                n_gal = len(group_membership[obj])\n",
    "                err = cluster_nstar_error(n_gal)\n",
    "                return pd.Series({'N_stars': n_gal * 1e11,\n",
    "                                  'log_N_stars': np.log10(n_gal * 1e11),\n",
    "                                  'log_N_stars_err': err})\n",
    "            elif t in ('G_Lens', 'GPair', 'GTrpl', 'GGroup', 'GClstr'):\n",
    "                n_gal = {'G_Lens': 1, 'GPair': 2, 'GTrpl': 3, 'GGroup': 4, 'GClstr': 10}.get(t, 1)\n",
    "                err = cluster_nstar_error(n_gal)\n",
    "                return pd.Series({'N_stars': n_gal * 1e11, 'log_N_stars': np.log10(n_gal * 1e11), 'log_N_stars_err': err})\n",
    "            else:\n",
    "                return pd.Series({'N_stars': 1e11, 'log_N_stars': 11., 'log_N_stars_err': 0.3})\n",
    "        nstars_cols = ned_df.apply(get_nstars, axis=1)\n",
    "        for c in nstars_cols.columns: ned_df[c] = nstars_cols[c]\n",
    "\n",
    "        d_m = ned_df['distance_mpc'].astype(float) * 3.086e22\n",
    "        log4pi = np.log10(4 * np.pi)\n",
    "        scaling_fraction = ned_df['scaling_factor'] / 100.0\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            ned_df['logEIRPmin'] = log4pi + 2 * np.log10(d_m) + np.log10(ned_df['fmin']) - np.log10(scaling_fraction)\n",
    "            ned_df['EIRPmin'] = 10 ** ned_df['logEIRPmin']\n",
    "            ned_df['log_Transmitter_Rate'] = - ned_df['log_N_stars'] - np.log10(ned_df['nu_rel'])\n",
    "            ned_df['CWTFM'] = (500 / 1e13) * ned_df['EIRPmin'] / (ned_df['N_stars'] * ned_df['nu_rel'])\n",
    "        # Unique summary (all types)\n",
    "        unique_objects = set(ned_df.loc[ned_df['distance_mpc'].notnull(), 'Object Name'])\n",
    "        for members in group_membership.values(): unique_objects.update(members)\n",
    "        summary_rows = []\n",
    "        for obj in unique_objects:\n",
    "            obj_rows = ned_df[(ned_df['Object Name'] == obj) & (ned_df['distance_mpc'].notnull())]\n",
    "            if obj_rows.empty: continue\n",
    "            row = obj_rows.iloc[0]\n",
    "            N_stars, log_N_stars, log_N_stars_err = row['N_stars'], row['log_N_stars'], row['log_N_stars_err']\n",
    "            median_dist = obj_rows['distance_mpc'].median()\n",
    "            avg_beam_response = obj_rows['scaling_factor'].mean()\n",
    "            median_dist_m = median_dist * 3.086e22 if not np.isnan(median_dist) else np.nan\n",
    "            if np.isnan(median_dist) or np.isnan(avg_beam_response) or avg_beam_response <= 0:\n",
    "                log_EIRPmin = np.nan; CWTFM = np.nan\n",
    "            else:\n",
    "                log_EIRPmin = log4pi + 2 * np.log10(median_dist_m) + np.log10(row['fmin']) - np.log10(avg_beam_response / 100)\n",
    "                EIRPmin = 10 ** log_EIRPmin\n",
    "                CWTFM = (500 / 1e13) * EIRPmin / (N_stars * row['nu_rel'])\n",
    "            log_TR = -log_N_stars - np.log10(row['nu_rel']) if N_stars > 0 else np.nan\n",
    "            log_TR_err = log_N_stars_err\n",
    "            summary_rows.append({\n",
    "                'field_name': row['field name'],\n",
    "                'object_name': obj,\n",
    "                'type': row['Type'],\n",
    "                'distance_mpc': median_dist,\n",
    "                'offset_arcmin': obj_rows['theta_arcmin'].mean(),\n",
    "                'beam_response_percent': avg_beam_response,\n",
    "                'N_stars': N_stars,\n",
    "                'log_Nstars': log_N_stars,\n",
    "                'log_Nstars_err': log_N_stars_err,\n",
    "                'log_EIRPmin': log_EIRPmin,\n",
    "                'log_TR': log_TR,\n",
    "                'log_TR_err': log_TR_err,\n",
    "                'CWTFM': CWTFM\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "        if summary_df.empty or 'distance_mpc' not in summary_df.columns:\n",
    "            if output_prefix:\n",
    "                summary_df.to_csv(f\"{output_prefix}_NED_catalog_summary.csv\", index=False)\n",
    "            return summary_df\n",
    "        summary_df = summary_df[(summary_df['distance_mpc'].notnull()) & (summary_df['distance_mpc'] <= limit_distance_mpc)]\n",
    "        if output_prefix:\n",
    "            summary_df.to_csv(f\"{output_prefix}_NED_catalog_summary.csv\", index=False)\n",
    "        return summary_df\n",
    "\n",
    "    # SPLIT BY BAND OR COMBINED\n",
    "    if split_by_band:\n",
    "        ned_df_dict = {}\n",
    "        type_counts_df_dict = {}\n",
    "        summary_df_dict = {}\n",
    "        for band in dataframe['receiving_band'].unique():\n",
    "            band_df = dataframe[dataframe['receiving_band'] == band].reset_index(drop=True)\n",
    "            print(f\"Processing receiving_band={band} with {len(band_df)} fields.\")\n",
    "            out_prefix = f\"{output_prefix}_{band}\" if output_prefix else None\n",
    "            ned_df, type_counts_df = ned_batch_query(band_df, band, out_prefix)\n",
    "            summary_df = ned_catalog_summary(\n",
    "                ned_df,\n",
    "                limit_distance_mpc=limit_distance_mpc,\n",
    "                output_prefix=out_prefix,\n",
    "                H0=H0,\n",
    "                ri_method=ri_method,\n",
    "                csv_path_for_ri=csv_path_for_ri\n",
    "            )\n",
    "            ned_df_dict[band] = ned_df\n",
    "            type_counts_df_dict[band] = type_counts_df\n",
    "            summary_df_dict[band] = summary_df\n",
    "        return ned_df_dict, type_counts_df_dict, summary_df_dict\n",
    "    else:\n",
    "        ned_df, type_counts_df = ned_batch_query(dataframe, \"all\", output_prefix)\n",
    "        summary_df = ned_catalog_summary(\n",
    "            ned_df,\n",
    "            limit_distance_mpc=limit_distance_mpc,\n",
    "            output_prefix=output_prefix,\n",
    "            H0=H0,\n",
    "            ri_method=ri_method,\n",
    "            csv_path_for_ri=csv_path_for_ri\n",
    "        )\n",
    "        return ned_df, type_counts_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50d102-f8e0-44e4-a240-300fe9af0aa5",
   "metadata": {},
   "source": [
    "# Uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfd601d6-b427-4d13-9241-a8a4b9362132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from s2sphere import LatLng, Cap, RegionCoverer, CellId, CellUnion, Angle, Cell\n",
    "\n",
    "def process_uno(\n",
    "    dataframe,\n",
    "    log_rho_dict={'lower': 8.443, 'nominal': 8.473, 'upper': 8.653},\n",
    "    log_rho_nominal=None, log_rho_pos_err=None, log_rho_neg_err=None,\n",
    "    split_by_band=False,\n",
    "    output_prefix=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    GSMF/Uno et al.-style SETI pipeline. Works on a DataFrame containing beams/fields.\n",
    "    Each field (row) must have: ['ra','dec','fwhm_arcmin','dlim_Mpc','nu_rel','fmin','field name','receiving_band']\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Columns as above.\n",
    "    log_rho_dict : dict, optional\n",
    "        IMF systematics for GSMF.\n",
    "    log_rho_nominal/pos_err/neg_err : float, optional\n",
    "        If provided, use these instead of log_rho_dict.\n",
    "    split_by_band : bool\n",
    "        If True, returns summary per unique receiving_band. If False, combines all bands as one.\n",
    "    output_prefix : str, optional\n",
    "        If provided, each summary DataFrame will be saved as a CSV file.\n",
    "    verbose : bool\n",
    "        Print status/info.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If split_by_band==False: summary_df (DataFrame, 1 row)\n",
    "        returns combined results.\n",
    "    If split_by_band==True: {band: summary_df} dict (one row per band)\n",
    "        Returns per-receiving_band DataFrames as dictionary entries.\n",
    "        access each DataFrame by summary_dict['band 1']...,\n",
    "    Notes\n",
    "    -------\n",
    "    - Resulting DataFrames utilize combined, overlap-subtracted total field stats using S2 union \n",
    "        (no overlap considered for just 1 field in Dataframe).\n",
    "    - dlim_Mpc, nu_rel, and fmin  must be the same for for all rows within each unique receiving band\n",
    "    - log_Mgal..., N_stars..., log_TR..., CWTFM... and errors all calculated from The 3 keys and \n",
    "        values in log_rho_dict, and by default log_rho_dict={'Chabrier (lower bound)': 8.443, \n",
    "        'Kroupa (nominal value)': 8.473, 'Salpeter (upper bound)': 8.653} from Uno et al. (2023).\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe.loc[:, 'field name'] = dataframe.loc[:, 'field name'].astype(str)\n",
    "    dataframe.loc[:, 'receiving_band'] = dataframe.loc[:, 'receiving_band'].astype(str)\n",
    "    dataframe.loc[:, 'ra'] = dataframe.loc[:, 'ra'].astype(float)\n",
    "    dataframe.loc[:, 'dec'] = dataframe.loc[:, 'dec'].astype(float)\n",
    "    dataframe.loc[:, 'fwhm_arcmin'] = dataframe.loc[:, 'fwhm_arcmin'].astype(float)\n",
    "    dataframe.loc[:, 'dlim_Mpc'] = dataframe.loc[:, 'dlim_Mpc'].astype(float)\n",
    "    dataframe.loc[:, 'fmin'] = dataframe.loc[:, 'fmin'].astype(float)\n",
    "    dataframe.loc[:, 'nu_rel'] = dataframe.loc[:, 'nu_rel'].astype(float)\n",
    "\n",
    "    def fwhm_fields_to_area_deg2_s2(ra_arr, dec_arr, fwhm_arr, max_cells=10000):\n",
    "        if len(ra_arr) == 1:\n",
    "            radius_deg = (fwhm_arr[0]/2) / 60.0\n",
    "            area_deg2 = np.pi * radius_deg**2\n",
    "            if verbose:\n",
    "                print(\"S2 union unnecessary for single field; using analytic area.\")\n",
    "            return area_deg2\n",
    "        cell_union = set()\n",
    "        coverer = RegionCoverer()\n",
    "        coverer.max_cells = max_cells\n",
    "        for ra, dec, fwhm in zip(ra_arr, dec_arr, fwhm_arr):\n",
    "            radius_arcmin = fwhm / 2\n",
    "            radius_rad = np.deg2rad(radius_arcmin / 60.0)\n",
    "            center = LatLng.from_degrees(dec, ra).to_point()\n",
    "            cap = Cap.from_axis_angle(center, Angle.from_radians(float(radius_rad)))\n",
    "            cells = coverer.get_covering(cap)\n",
    "            for cell in cells:\n",
    "                cell_union.add(cell.id())\n",
    "        cell_union = CellUnion(list(cell_union))\n",
    "        area_sr = sum(Cell(i).get_rect_bound().area() for i in cell_union.cell_ids())\n",
    "        area_deg2 = area_sr * (180.0/np.pi)**2\n",
    "        return area_deg2\n",
    "\n",
    "    def uno_summary_from_df(df, save_path=None):\n",
    "        ra_arr = df['ra'].astype(float).to_numpy()\n",
    "        dec_arr = df['dec'].astype(float).to_numpy()\n",
    "        fwhm_arr = df['fwhm_arcmin'].astype(float).to_numpy()\n",
    "        dlim_Mpc = df['dlim_Mpc'].astype(float).iloc[0]\n",
    "        nu_rel = df['nu_rel'].astype(float).iloc[0]\n",
    "        fmin = df['fmin'].astype(float).iloc[0]\n",
    "        total_area_deg2 = fwhm_fields_to_area_deg2_s2(ra_arr, dec_arr, fwhm_arr)\n",
    "        Omega_tot = total_area_deg2 * (np.pi/180)**2\n",
    "        k_tot = Omega_tot / 3\n",
    "        Vr_tot = k_tot * dlim_Mpc**3\n",
    "\n",
    "        use_nominal_mode = (log_rho_nominal is not None)\n",
    "        if use_nominal_mode:\n",
    "            log_Mgal_nom = log_rho_nominal + np.log10(Vr_tot)\n",
    "            pos_err = log_rho_pos_err if log_rho_pos_err is not None else 0\n",
    "            neg_err = log_rho_neg_err if log_rho_neg_err is not None else 0\n",
    "            log_Mgal_upper = log_Mgal_nom + pos_err\n",
    "            log_Mgal_lower = log_Mgal_nom - neg_err\n",
    "        else:\n",
    "            log_Mgal = {IMF: log_rho + np.log10(Vr_tot) for IMF, log_rho in log_rho_dict.items()}\n",
    "            log_Mgal_nom = log_Mgal['nominal']\n",
    "            log_Mgal_upper = log_Mgal['upper']\n",
    "            log_Mgal_lower = log_Mgal['lower']\n",
    "            pos_err = log_Mgal_upper - log_Mgal_nom\n",
    "            neg_err = log_Mgal_nom - log_Mgal_lower\n",
    "        N_stars_nom = 10**log_Mgal_nom\n",
    "        N_stars_upper = 10**log_Mgal_upper\n",
    "        N_stars_lower = 10**log_Mgal_lower\n",
    "        log_TR = -log_Mgal_nom - np.log10(nu_rel)\n",
    "        log_TR_pos_err = neg_err\n",
    "        log_TR_neg_err = pos_err\n",
    "        d_m = dlim_Mpc * 1e6 * 3.086e16\n",
    "        log_4pi = np.log10(4 * np.pi)\n",
    "        log_EIRPmin = log_4pi + 2 * np.log10(d_m) + np.log10(fmin)\n",
    "        EIRPmin = 10**log_EIRPmin\n",
    "        CWTFM_nom = (500 / 1e13) * EIRPmin / (N_stars_nom * nu_rel)\n",
    "        CWTFM_upper = (500 / 1e13) * EIRPmin / (N_stars_upper * nu_rel)\n",
    "        CWTFM_lower = (500 / 1e13) * EIRPmin / (N_stars_lower * nu_rel)\n",
    "        CWTFM_pos_err = CWTFM_lower - CWTFM_nom\n",
    "        CWTFM_neg_err = CWTFM_nom - CWTFM_upper\n",
    "        row = {\n",
    "            'area_deg2': total_area_deg2,\n",
    "            'dlim_Mpc': dlim_Mpc,\n",
    "            'Vr_Mpc3': Vr_tot,\n",
    "            \"nu_rel\": nu_rel,\n",
    "            'fmin': fmin,\n",
    "            'log_Mgal': log_Mgal_nom,\n",
    "            'log_Mgal_pos_err': pos_err,\n",
    "            'log_Mgal_neg_err': neg_err,\n",
    "            'N_stars': N_stars_nom,\n",
    "            'N_stars_upper': N_stars_upper,\n",
    "            'N_stars_lower': N_stars_lower,\n",
    "            'log_TR': log_TR,\n",
    "            'log_TR_pos_err': log_TR_pos_err,\n",
    "            'log_TR_neg_err': log_TR_neg_err,\n",
    "            'log_EIRPmin': log_EIRPmin,\n",
    "            'CWTFM': CWTFM_nom,\n",
    "            'CWTFM_pos_err': CWTFM_pos_err,\n",
    "            'CWTFM_neg_err': CWTFM_neg_err\n",
    "        }\n",
    "        summary_df = pd.DataFrame([row])\n",
    "        if save_path is not None:\n",
    "            summary_df.to_csv(save_path, index=False)\n",
    "        return summary_df\n",
    "\n",
    "    if split_by_band:\n",
    "        summary_dict = {}\n",
    "        for band, band_df in dataframe.groupby('receiving_band'):\n",
    "            if verbose:\n",
    "                print(f\"Processing receiving_band={band} with {len(band_df)} fields.\")\n",
    "            save_path = f\"{output_prefix}_{band}_uno.csv\" if output_prefix else None\n",
    "            summary_dict[band] = uno_summary_from_df(band_df, save_path=save_path)\n",
    "        return summary_dict\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Processing all bands together ({len(dataframe)} fields).\")\n",
    "        save_path = f\"{output_prefix}_uno.csv\" if output_prefix else None\n",
    "        return uno_summary_from_df(dataframe, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e08dea-76a7-4ee1-a990-df21c3b28fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SETIvenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
